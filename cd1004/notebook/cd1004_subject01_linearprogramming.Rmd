---
title: "Subject 01 - Linear Programming"
author:
-  Juan Carlos MartÃ­nez-Ovando
-  jc-mo@tec.mx
date: "Feb-Jun 2020"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    number_sections: yes
    self_contained: yes
    theme: cerulean
    highlight: textmate
fig_align: "center"
fig_width: 18
---

# Objetives  {.tabset .tabset-fade .tabset-pills}

* Conceptually describing in what situations the linear programming method can be applied, and what this quantitative tool consists of.

* Describe the general steps to follow in solving linear programming problems.

* Determine the practical elements necessary to implement a solution with the linear programming method.

---

# **Motivating case study**

> **A resource allocation problem**

A company produces copper cable of 5 and 10 mm of diameter on *a
single production line* with the following constraints:

The production capacity of the company allows to produce up to 104.4 meters --under the assumtion of having a linear production line of both cables produced--. The company must produce 10mm diameter cable in a 60 percent proportion, with respect to the producion ot 5mm diameter cable.

Product distribution consideration allow the company to produce at most 200 meters of both types of cable combined, since it is now allowed to store the excedent of cable not distributed within the week.

Due to demand, the market can take up to 102.8571 meters of both types of cables, among which 42.85714 percent must be of 10 mm diameter cable, and 57.14286 percent of 5mm diameter cable.

Both cables are respectively sold at 350 and 300 EUR the meter.

> Question: **What amount of meters must the company produce of both types of cables in order to maximize its weekly revenue?**

## Some other examples

> Let us take a few minutes to formulate similar to the `motivating case study` based in our everyday life.

---

# Steps for **modelling in practice**

1. **Defining the Problem**

2. **Developing a Model**

3. **Acquiring Input Data**

4. **Developing a Solution**

5. **Testing the Solution**

6. **Analyzing the Results**

7. **Communicating the Solution and Results**

8. **Implementing the Results**

# Introduction to **optimization**

## Montivation

Why **linear programming** is a very important topic?

* A lot of problems can be formulated as linear programmes.

* Nowadays, here exist efficient methods to solve them -- or at least give good approximations--.

## What is a **linear programming problem**?

It is an *optimization problem* consisting in

a. maximizing (or minimizing) a `linear objective` function of $n$ decision variables

b. subject to a set of constraints expressed by linear equations or
inequalities.

## History

It was originated within a military context: 

> `programme` = `resource planning`

The terminology is due to *George B. Dantzig*, inventor of the Simplex
Algorithm (1947), developed during WWII and published right afterwards.

## Terminology


a. **Decision variables** are denoted by $x_1,x_2,\ldots,x_n$.

b. **Objetive function** mapping $\Re^n\rightarrow \Re$ in a linear way.

c. **Constraints** related to the $n$ decision variables. These constraints are typically expressed in the form of linear inequalities.

> The **optimization rule** consist in `maximizing (or minimizing)` the `objective function` with regards to the $n$ `decision variables` subject to the given constraints.

## Remarks

Recall that in **linear programming**:

a. Both `objetive function` & `constraints` **must all be linear**

b. Typically, `objetive variables` are all possitive.

c. If `objective variables` are integers, then the problem turns into **Integer Programme (IP)**, which is far more challenging.


---

# Example: **Motivating case study**

Lets consider **two decision variables**, 
* $x_1$ - meters of 10mm cable, and 
* $x_2$ - meters of 5mm cable, 

and an **objetvice function** given by 
$$
f(x_1,x_2)=350 x_1 + 300 x_2.
$$

> The **optimization rule** consists in *maximizing* $f(x_1,x_2)$, i.e. finding such that
$$
(x_1^*, x_2^*)
 = 
 \arg\max_{(x_1,x_2)\in \mathbb{R}^{2}} f(x_1,x_2),
$$
subjet to the follow collection of **constraints**:
\begin{eqnarray}
x_1 + x_2 & \leq & 200 \nonumber \\
0.6 x_1 + 0.4 x_2 & \leq & 104.4 \nonumber \\
0.4285714 x_1 + 0.5714286 x_2 & \leq & 102.8571 \nonumber \\
x_1 & \geq & 0 \nonumber \\
x_2 & \geq & 0. \nonumber
\end{eqnarray}

> Alternatively, the constrainst cabe expressed as follows:
\begin{eqnarray}
x_1 + x_2 & \leq & 200 \nonumber \\
9 x_1 + 6 x_2 & \leq & 1566 \nonumber \\
12 x_1 + 16 x_2 & \leq & 2880 \nonumber \\
x_1 & \geq & 0 \nonumber \\
x_2 & \geq & 0. \nonumber
\end{eqnarray}


## Graphical soluction

* The constraints of a linear programme define a zone of solutions.

* The best point of the zone corresponds to the optimal solution.

* For problem with 2 variables, easy to draw the zone of solutions and to find the optimal solution graphically.

### First constraint

\begin{eqnarray}
x_1 + x_2 & \leq & 200 \nonumber \\
\end{eqnarray}

![Layout of the first constraint](./figures/cd2007_week1_const1.png)

### First + second constraint

\begin{eqnarray}
x_1 + x_2 & \leq & 200 \nonumber \\
9 x_1 + 6 x_2 & \leq & 1566 \nonumber \\
\end{eqnarray}

![Layout of the first + second constraints](./figures/cd2007_week1_const1+2.png)

### First + second + third constraint

\begin{eqnarray}
x_1 + x_2 & \leq & 200 \nonumber \\
9 x_1 + 6 x_2 & \leq & 1566 \nonumber \\
12 x_1 + 16 x_2 & \leq & 2880 \nonumber \\
\end{eqnarray}

![Layout of the first + second + third constraints](./figures/cd2007_week1_const1+2+3.png)

### Over imposing the `objetive function`

\begin{eqnarray}
f(x_1,x_2) = 350 x_1 + 300 x_2 & \leq & 200
\end{eqnarray}

![Layout over impossing the objctive function at $f(x_1,x_2)=35,000$](./figures/cd2007_week1_const1+2+3+of.png)


![Layout over impossing the objctive function  at $f(x_1,x_2)=52,500$](./figures/cd2007_week1_const1+2+3+of2.png)

### Optimal Solution
The **optimal solution** is reached at $52,500$

![Layout over impossing the objctive function  at $f(x_1,x_2)=52,500$](./figures/cd2007_week1_const1+2+3+of_opt.png)


## Analytic solution

The computation of the optimal solution requires the **identification** of the intersection of the constraints, i.e.
\begin{eqnarray}
x_1+x_2 & = &  200 \nonumber \\
9x_1+6x_2 & = &  1566 \nonumber
\end{eqnarray}

Thus, we get that
\begin{eqnarray}
x^*_1 & = & 122 \nonumber \\
x^*_2 & = & 78, \nonumber
\end{eqnarray}
for which
$$
f(x^*_1,x^*_2) = 66,100.
$$

# Remarks

Optimal solutions to **linear programme** problems might turn into the follow types:

![Types of optimal solutions](./figures/cd2007_week1_opt_types.png)

a. A **single** optimal solution.

b. An **infinite number** of optimal solution.

c. **No optimal** solution, at all.

> If an optimal solution exists, there is always a **corner point** optimal solution!

---

# **Solving** linear programming problems

> I. The constraints of an LP give rise to a geometrical shape: a **polyhedron**.

> II. If we can determine **all the corner points** of the polyhedron, then we calculate the objective function at these points and take the best one as our optimal solution.

> III. The **Simplex Method** intelligently moves from corner to corner until it can prove that it has found the optimal solution.

## Notes

* Geometric method impossible in higher dimensions

* Algebraical methods: **Simplex method** skim through the feasible solution polytope. Similar to a "Gaussian elimination". Very good in practice, but can take an exponential time.

* Polynomial methods exist: **ellipsoid method**. But more theoretical than practical.

## Integer programme

* Feasible region: a set of discrete points.

* Corner point solution not assured.

* No "efficient" way to solve an IP.

* Solving it as an LP provides a relaxation and a bound on the solution.

# **Little-big-insignificant details**

## Difficulty: Large number of solutions.

* `Choose the best solution` among $2n$ or $n!$ possibilities: *all solutions cannot be enumerated*

* `Complexity of studied problems`: often NP-complete.

## Solving methods:

* Optimal solutions:

 - Graphical method (2 variables only).

 - Simplex method.

* Approximations:

 - Theory of duality (assert the quality of a solution).

 - Approximation algorithms.

---
---

## **Remembering**

a. What is a linear programme.

b. The graphical method of resolution.

c. Linear programs can be solved efficiently (polynomial).

* **Why integer programs** are so harder (in general no polynomial algorithms). *In this case, we look for approximate solutions.*

## Steps to **modelling in practice**

1. **Defining the Problem**

2. **Developing a Model**

3. **Acquiring Input Data**

4. **Developing a Solution**

5. **Testing the Solution**

6. **Analyzing the Results**

7. **Communicating the Solution and Results**

8. **Implementing the Results**

# Linear programming formulation 

## The structure of a linear programming model model

As mentioned in the previous week, a `linear programming` problem consists in:

a. The `optimization` of an **linear objetive function** of `decision variables`

b. The `optimization` shall consider admissible values defined by a set of **linear constraints**.

Thus, in general, we need to `structure` (a.k.a. translate, if possible) the **problem** into the form
\begin{eqnarray}
\max (or \min) f(x_1,\ldots,x_n) & = & a_1 x_1 + \cdots + a_n x_n \nonumber \\
\text{such that} & & c_{11} x_1 + \cdot + c_{1n} x_n  \leq l_1 \nonumber \\   
 & & c_{21} x_1 + \cdots + c_{2n} x_n  \leq l_2 \nonumber \\   
 & & \vdots \nonumber \\
 & & c_{m1} x_1 + \cdots + c_{mn} x_n  \leq l_m \nonumber \\   
 & & x_n \geq 0, \ \text{ for all } \ n.
\end{eqnarray}

> Thus, in order to **solve** a **real life problem** using `linear programming`, we need to be able to **translate** the `problem` into general structure of LP.

> **Recall:** Not all decision problems can be structured as a `LP` model. And not all `LP` model can have a unique solution --nor a solution, at all--.

# A Simple Example

A small business sells two products, named `Product 1` and `Product 2`.

Each tonne of `Product 1` consumes 30 working hours, and each tonne of `Product 2` consumes 20 working hours. 

The business has a maximum of `2,700 working hours` for the period considered. As for machine hours, each tonne of `Products 1` and `2` consumes `5` and `10` machine hours, respectively. There are 850 machine hours available.

Each tonne of `Product 1` yields `20 Me` of profit, while `Product 2` yields `60 Me` for each tonne sold. For technical reasons, the firm must produce a minimum of 95 tonnes in total between both products. 

> **Problem:** We need to know how many tonnes of `Product 1` and `Product 2` must be produced to *maximize total profit*.

> This situation is apt to be modeled as a `PL` model.

## Formulation

a. Defining the **decision variables**:

$x_1$ - number of tonnes produced and sold of `Product 1`

$x_2$ -  number of tonnes produced and sold of `Product 2`

**Note: ** The cost coefficients of these variables are $20$ and $60$, respectively. Therefore, the **objective function** is defined multiplying each variable by itscorresponding cost coefficient.

b. The **constraints** of this `LP` model are:
 
   - A constraint WH making that the total amount of working hours used in Product 1 and Product 2, which equals 
 $$30 x_1 + 20 x_2 \leq 2700,$$ hours.

   - A similar constraint MH making that the total machine hours
$$5 x_1 + 10 x_2 \leq 850.$$

   - A `PM` constraint making that the total units produced and sold
$$x_1 + x_2 \geq 95.$$

c. Putting all this considerations together, the `LP` that maximizes profit is:

\begin{eqnarray}
\max f(x_1,x_2) & = & 20x_1 + 60x_2 \nonumber \\
s.t. & &  \text{(a)} \ \ 30x_1 + 20x_2 \leq 2700 \nonumber \\
     & &  \text{(b)} \ \  5x_1 + 10x_2 \leq 850 \nonumber \\
     & &  \text{(c)} \ \  x_1 + x_2 \geq 95 \nonumber \\
     & &  \text{(d)} \ \  x_1 \geq 0; x_2 \geq 0. \nonumber
\end{eqnarray}

## Solution using `lpSolve`

The package `lpSolve` is one among several packages developed in `R` to solve `LP` models in medium to large scale.

The follow instructions are used in `R` to check if `lpSolve` is installed already in the OS. If it is not previously installed, then it the script calls for its installation.


```{r}
if(!require("lpSolve")){install.packages("lpSolve")}
library("lpSolve")
```

The instruction
```
library("lpSolve")
```

is intended to call and load the package `lpSolve` for use.

## Defining parameters

Once `lpSolve` is being installed and loaded, we shall proceed with the definition of the `domain variables`:

```{r}
obj.fun <- c(20 , 60)
```

The values `20` and `60` are the coefficients associated with the **objetive function**:
$$
f(x_1,x_2) = 20x_1 + 60x_2
$$

> Be ware that the order in which these parameters are stated is important. for instance, defining `obj.fun2 <- c(60 , 20)` would rise to a whole different objective function 
$$
f_2(x_1,x_2) = 60x_1 + 20x_2.
$$

The  we proceed with the definition of the **constraints**. 

* In this particular case, the coefficients `30`, `20` are associated with the **constraint (a)** given by
$$
30x_1 + 20x_2.
$$

* Likewise, the coefficients (`5`,`10`) and (`1`,`1`) are devoted to define **constraints (b) and (c)**, respectively.

$$
5x_1 + 10x_2,\\
x_1 + x_2.
$$


```{r}
constr <- matrix(c(30 , 20, 
                    5, 10, 
                    1, 1), 
                  ncol = 2, byrow = TRUE )
```

Directions of each *constraint* must be set in a list, as follows:

```{r}
constr.dir <- c("<=", 
                "<=", 
                ">=")
```

**Constraints** are completeley specified by the definition of the **limits** or **bounds**, as follows:

```{r}
rhs <- c(2700, 
         850, 
         95)
```

> This figures correspon to 
$$
l_1 = 2700 \\
l_2 = 850 \\
l_3 = 95,
$$
in the general specification, associated with **constraints (a), (b) and (c)**, respectively,

---

> **NOTE** that in `R`/`RStudio` the **objetive function** and **constraints** must be defined in a **refined grid** instead of being defined as a **continuous function**.

---

## Solving the `LP` model

The solution of the `LP` model in `R` using the package `lpSolve` is implemented trough the function `lp()`, whose arguments/inputs are:

* Optimization rule (either `max` or `min`). In our case, it is `"max"`.

* Objective function, defined in a grid. In this case, it is the object `obj.func`

* Set of constraint coefficients. In this case, those correspond to the object `constr.dir`

* Set of *bounds7constraint limits*, which correspond to `rhs` in this case.

```{r}
prod.sol <- lp("max", 
                obj.fun, 
                constr,
                constr.dir, 
                rhs,
                compute.sens = TRUE )
```

The solution produces the object `prod.sol` with the follow attributes:

```{r}
ls(prod.sol)
```

> The **maximum total profit** is reached at **4,900**, which can be accessed by symply typing `prod.sol`.

```{r}
prod.sol
```

> To know the **maximum value of the objective function**:

```{r}
prod.sol$objval
```

> The values of the **decision variables** that **mazimizes profit** are:

```{r}
prod.sol$solution
```

## Main result

> That is to say, the **company** must produce and sold

* `20` tonnes of `Product 1`

* `75` tonnes of `Prodcut 2`

> in order to achieve **maximum profit at** `4,900`.

